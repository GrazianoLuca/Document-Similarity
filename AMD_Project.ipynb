{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ],
      "metadata": {
        "id": "G3BubcCiGErX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "388bfdbd-d2bd-4e9f-abf0-452eb1739c0d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=984300fbf2ba8f7526e5238530bd4030a121a3d642739f1daef0fa13abbcff93\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import os\n",
        "import pyspark\n",
        "import findspark\n",
        "import pandas as pd\n",
        "import csv \n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "_L9BV27DEtsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41862a33-d1df-416b-f2bd-ac025a0091d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kaggle keys\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"lucagee\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"a2fbb8660c3567ac94eb3e66d1619953\""
      ],
      "metadata": {
        "id": "jrhE1prbGMtI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download medal dataset\n",
        "\n",
        "ref = \"xhlulu/medal-emnlp\"\n",
        "\n",
        "!kaggle datasets download $ref --unzip -p .\n"
      ],
      "metadata": {
        "id": "GuBVOf95HMaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "205ce5bd-3374-48ae-cbb4-f1c778e348fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading medal-emnlp.zip to .\n",
            "100% 6.80G/6.82G [00:46<00:00, 160MB/s]\n",
            "100% 6.82G/6.82G [00:46<00:00, 158MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Spark environment\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "kPNHjcqSBTTY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "\n",
        "df = spark.read.csv(\"full_data.csv\", header = True)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "vrQ8VeuGI6wM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60dcff26-b5ec-4e14-9a48-6154ddd2d647"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|                TEXT|            LOCATION|               LABEL|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|alphabisabolol ha...|                  56|           substrate|\n",
            "|a report is given...|24|49|68|113|137|172|carcinosarcoma|re...|\n",
            "|the virostatic co...|                  55|           substrate|\n",
            "|rmi rmi and rmi a...|   25|82|127|182|222|compounds|compoun...|\n",
            "|a doubleblind stu...|22|26|28|77|90|14...|oxazepam|placebo|...|\n",
            "|stroma from eithe...|         6|82|84|107|red cells|serum|a...|\n",
            "|the effect of the...|                4|13|major|pentose pho...|\n",
            "|in one experiment...|        32|44|76|135|feeding|feeding|a...|\n",
            "|the presence of a...|7|15|63|137|199|2...|active|study|acti...|\n",
            "|the reaction of g...|     113|203|209|250|stable|assay|bind...|\n",
            "|choline acetyltra...|                  44|             caudate|\n",
            "|increasing concen...|                  81|        displacement|\n",
            "|the properties of...|         5|35|98|257|groups|functional...|\n",
            "|primary amines re...|          95|164|183|arginine|arginine...|\n",
            "|a purification pr...|     9|42|70|100|206|dihydrofolate|spe...|\n",
            "|dihydrofolate red...|71|106|118|130|15...|gel filtration|el...|\n",
            "|ionization effect...|               8|129|transition state|...|\n",
            "|kinetic analyses ...|100|155|161|254|2...|effective|inhibit...|\n",
            "|the nearultraviol...|                 166|        heterologous|\n",
            "|the circular pola...|                 248|polysaccharide an...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "\n",
        "- Na removal\n",
        "- making text case insesitive"
      ],
      "metadata": {
        "id": "ve4EcwAjGe6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop useless variables\n",
        "\n",
        "df = df.drop(\"LOCATION\", \"LABEL\")"
      ],
      "metadata": {
        "id": "4SICfeCWB7QW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop Na values and duplicates\n",
        "\n",
        "df.na.drop()\n",
        "df.dropDuplicates()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7Il5HZxEt1Z",
        "outputId": "38db904b-2ccf-4e1c-887c-3cb36b4303b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[TEXT: string]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of useful variables for cleaning\n",
        "\n",
        "def remove_digits(tweet):\n",
        "  '''\n",
        "    Function that takes as input a string and removes all digits \n",
        "  '''\n",
        "  tweet = re.sub(r'[0-9]', '', tweet)\n",
        "\n",
        "  return tweet\n",
        "\n",
        "def remove_punctuation(tweet):\n",
        "\n",
        "\n",
        "    import string\n",
        "    '''\n",
        "      Function that takes as input a tweet and removes all punctuation\n",
        "    '''\n",
        "\n",
        "    tweet = tweet.translate(\n",
        "      str.maketrans(string.punctuation + \"…’”“‘\", \" \" * (len(string.punctuation) + 5))\n",
        "  )\n",
        "\n",
        "    return tweet\n",
        "\n",
        "def remove_extra_spaces(tweet):\n",
        "\n",
        "  '''\n",
        "    Function that takes as input a tweet and removes all extra white space\n",
        "  '''\n",
        "\n",
        "  tweet = \" \".join(tweet.strip().split())\n",
        "  \n",
        "  return tweet\n",
        "\n",
        "def remove_stopwords(tweet):\n",
        "\n",
        "  '''\n",
        "    Function that takes as input a tweet and removes all English stopwords\n",
        "  '''\n",
        "  from nltk.corpus import stopwords\n",
        "\n",
        "  stop_words = list(set(stopwords.words(\"english\")))\n",
        "  [stop_words.append(w) for w in [\"lol\", \"mkr\", \"http\", \"andre\", \"kat\", \"co\", \"rt\", \"oh\"]]  # Adding useless words that seemed to come up very often\n",
        "\n",
        "  output =  ' '.join([word for word in tweet.split() if word not in stop_words])\n",
        "  \n",
        "  return output\n",
        "\n",
        "\n",
        "def cleaning(tweet):\n",
        "\n",
        "  ''' \n",
        "    Function that takes as input a tweet and applies all preprocessing functions required to fully clean it\n",
        "  '''\n",
        "\n",
        "  tweet = tweet.lower() # standardise all tweet to lower case\n",
        "\n",
        "  tweet = remove_digits(tweet)\n",
        "  tweet = remove_extra_spaces(tweet)\n",
        "  tweet = remove_punctuation(tweet)\n",
        "  tweet = remove_stopwords(tweet)\n",
        "\n",
        "  return tweet\n"
      ],
      "metadata": {
        "id": "P6FlEgaSE32M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, udf\n",
        "\n",
        "cleanUDF = udf(lambda x: cleaning(x)) \n",
        "df = df.withColumn(\"text\", cleanUDF(col(\"TEXT\")))"
      ],
      "metadata": {
        "id": "7jMyM-27IZyJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add column ID\n",
        "\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "df = df.withColumn(\"id\", monotonically_increasing_id())"
      ],
      "metadata": {
        "id": "CC7ckELrqr7g"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to RDD\n",
        "\n",
        "rdd = df.rdd"
      ],
      "metadata": {
        "id": "lGhvQ_wfzFfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shingling"
      ],
      "metadata": {
        "id": "PNuUvHkWR97x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shingle_length = 4"
      ],
      "metadata": {
        "id": "UCf-06sZxHNx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shingles = rdd.flatMap( \n",
        "                  lambda x: [(x.id, x.text[i : i + shingle_length]) for i in range(len(x.text) - shingle_length)])#.distinct()\n",
        "\n",
        "shingles.take(5)"
      ],
      "metadata": {
        "id": "Fbb8_C-UwavJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QevMVfL2SPm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FP4vyGnXW9P5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}